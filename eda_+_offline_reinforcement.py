# -*- coding: utf-8 -*-
"""eda + offline_reinforcement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RQSAXS6b5-pn29v0GEZGUqfggrN_qLDn

# Initial cleaning of dataset (similar to supervised model cleaning)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

df = pd.read_csv('accepted_2007_to_2018Q4.csv', low_memory=False)

# Filtering relevant loan statuses
final_status = ['Fully Paid', 'Charged Off', 'Default']
df = df[df['loan_status'].isin(final_status)].copy()

# Creating binary targets
df['target'] = df['loan_status'].apply(lambda x: 1 if x in ['Charged Off', 'Default'] else 0)

# Drop columns with >50% missing values
missing_ratio = df.isnull().mean()
df.drop(columns=missing_ratio[missing_ratio > 0.5].index, inplace=True)

# Drop irrelevant text columns
drop_cols = ['member_id', 'id', 'url', 'title', 'zip_code', 'issue_d',
             'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d',
             'emp_title', 'loan_status', 'pymnt_plan', 'initial_list_status',
             'disbursement_method']
df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)

# Encode binary flags
binary_map = {'Y': 1, 'N': 0}
for col in ['hardship_flag', 'debt_settlement_flag']:
    if col in df.columns:
        df[col] = df[col].map(binary_map)

# Fill missing values
df['emp_length'] = df['emp_length'].fillna('Unknown')
df['dti'] = df['dti'].fillna(df['dti'].median())
df['revol_util'] = df['revol_util'].fillna(df['revol_util'].median())

# Ordinal encoding
df['term'] = df['term'].apply(lambda x: 0 if '36' in x else 1)
grade_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}
df['grade'] = df['grade'].map(grade_map)

# Clean emp_length
import re
def clean_emp_length(x):
    if x == 'Unknown': return 0
    match = re.findall(r'\d+', str(x))
    return int(match[0]) if match else 0
df['emp_length'] = df['emp_length'].apply(clean_emp_length)

# One-hot encode categorical columns
categorical_cols = ['home_ownership', 'verification_status', 'purpose', 'addr_state', 'application_type']
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Final cleanup: drop any remaining object columns
df.drop(columns=df.select_dtypes(include='object').columns, inplace=True)

print(df.columns.tolist())

# Save full cleaned DataFrame including target
df.to_csv('cleaned_loan_data_with_target.csv', index=False)

"""# Using the cleaned dataset for Reinforcement learning"""

import pandas as pd
df = pd.read_csv('cleaned_loan_data_with_target.csv')

df_rl = df.copy()

# Action: always 1 (approved)
df_rl['action'] = 1

# Reward:
df['reward'] = df.apply(
    lambda row: row['loan_amnt'] * row['int_rate'] / 100 if row['target'] == 0 else -row['loan_amnt'],
    axis=1
)

# State: drop target, reward, action
state_cols = [c for c in df_rl.columns if c not in ['target', 'reward', 'action']]

"""# Normalizing Features"""

reward_scale = 10000.0
df['reward'] = df.apply(
    lambda row: (row['loan_amnt'] * row['int_rate'] / 100) / reward_scale if row['target'] == 0
    else -row['loan_amnt'] / reward_scale,
    axis=1
)

# Checking how many columns still have NaN values
nan_cols = df[state_cols].isnull().sum()
print(nan_cols[nan_cols > 0])

# Replacing NaN values based on whether it is numerical or categorical data
for col in df[state_cols].columns:
    if df[col].isnull().any():
        if df[col].dtype == 'float' or df[col].dtype == 'int':
            df[col] = df[col].fillna(df[col].median())
        else:
            df[col] = df[col].fillna('Unknown')

# Confirming there is no more NaN values in columns
assert not df[state_cols].isnull().any().any(), "Still NaNs after filling!"

# Scaling features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[state_cols])

# Confirming there is no NaN values after scaling
assert not np.isnan(X_scaled).any(), "NaNs in scaled features!"

"""# Training the RL Agent using DQN (Deep Q-Network)"""

from d3rlpy.dataset import MDPDataset
from d3rlpy.algos import CQL
from sklearn.model_selection import train_test_split
import numpy as np

# Preparing the  dataset
X_scaled = df[state_cols].values.astype(np.float32)
actions = np.ones(len(df), dtype=np.int32)
rewards = df['reward'].values.astype(np.float32)
terminals = np.ones_like(rewards, dtype=bool)

# Splitting the dataset for training, validation and testing
from sklearn.model_selection import train_test_split

X_train, X_val, a_train, a_val, r_train, r_val, t_train, t_val, train_indices, X_val_indices = train_test_split(
    X_scaled, actions, rewards, terminals, range(len(X_scaled)), test_size=0.1, random_state=42
)

"""# Test approach
Creating an MDP dataset for training the reinforcement learning agent
Stores:
- States (X)
- actions (a)
- rewards (r)
- terminal flags (t)
"""

from d3rlpy.dataset import MDPDataset

dataset = MDPDataset(X_train, a_train, r_train, t_train)

"""# Testing the DQN training pipeline to check whether dataset structure is valid or not"""

from d3rlpy.algos import DQN, DQNConfig

config = DQNConfig()

# Initialize agent with required arguments
dqn = DQN(config=config, device='cpu', enable_ddp=False)

dqn.fit(dataset, n_steps=10)

"""# Predicting actions from the dataset"""

predicted_actions = dqn.predict(X_val)

"""# Evaluating the predicted policy on validation data by computing expectated reward (realistic reward function)

Reward logic:
- If loan is approved and customer repays (target=0): positive profit = loan_amount * interest.
- If loan is approved and customer defaults (target=1): loss = -loan_amount.
- If loan is denied: no profit/loss = 0.
"""

# Reconstructing validation DataFrame
val_df = df.iloc[X_val_indices].copy()

# Applying predicted actions
val_df['predicted_action'] = predicted_actions

# Computing reward based on predicted action (exploratory step)
def compute_reward(row):
    # Loan approved
    if row['predicted_action'] == 1:
        return row['loan_amnt'] * row['int_rate'] / 100 if row['target'] == 0 else -row['loan_amnt']

    # Loan denied
    else:
        return 0.0

val_df['predicted_reward'] = val_df.apply(compute_reward, axis=1)

"""# Computing average expected reward from the validation predictions"""

average_reward = val_df['predicted_reward'].mean()
print("Average Predicted Reward:", average_reward)

"""# Checking how many approvals vs denials the agent predicted - to assess policy bias"""

print(val_df['predicted_action'].value_counts())

"""# Analyzing average reward per predicted action"""

val_df.groupby('predicted_action')['predicted_reward'].mean()

"""# Reward system for the DQN agent to learn a general policy for loan approval (simplified reward definition for RL training)"""

df['reward'] = df['target'].apply(lambda x: 1.0 if x == 0 else -1.0)

# Assuming all historical loans were approved
df['action'] = 1

"""# Creating a synthetic dataset for both possible actions:
- approved_df: historical real loans (action=1)
- denied_df: hypothetical 'deny' cases (action=0, zero reward)
"""

approved_df = df.copy()
denied_df = df.copy()

approved_df['action'] = 1
approved_df['reward'] = approved_df['target'].apply(lambda x: 1.0 if x == 0 else -1.0)

denied_df['action'] = 0

# no loan = no gain/loss
denied_df['reward'] = 0.0

combined_df = pd.concat([approved_df, denied_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)

"""# Preparing numerical features for training"""

state_cols = [c for c in combined_df.columns if c not in ['target', 'reward', 'action']]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(combined_df[state_cols])
actions = combined_df['action'].values.astype(np.int32)
rewards = combined_df['reward'].values.astype(np.float32)
terminals = np.ones_like(rewards, dtype=bool)

"""# Splitting the dataset into training and validation sets for model evaluation"""

from sklearn.model_selection import train_test_split

X_train, X_val, a_train, a_val, r_train, r_val, t_train, t_val, train_indices, val_indices = train_test_split(
    X, actions, rewards, terminals, range(len(X)), test_size=0.1, random_state=42
)

"""# Creating an MDP dataset for training the reinforcement learning agent
Stores:
- States (X)
- actions (a)
- rewards (r)
- terminal flags (t)
"""

from d3rlpy.dataset import MDPDataset

dataset = MDPDataset(X_train, a_train, r_train, t_train)

"""# Training the DQN agent on the offline loan dataset"""

config = DQNConfig()

# Initializing agent with required arguments
dqn = DQN(config=config, device='cpu', enable_ddp=False)

dqn.fit(dataset, n_steps=50000)

"""# Evaluation metric"""

predicted_actions = dqn.predict(X_val)
val_df = combined_df.iloc[val_indices].copy()
val_df['predicted_action'] = predicted_actions

def compute_reward(row):
    if row['predicted_action'] == 1:
        return 1.0 if row['target'] == 0 else -1.0
    else:
        return 0.0

val_df['predicted_reward'] = val_df.apply(compute_reward, axis=1)
print("Average Predicted Reward:", val_df['predicted_reward'].mean())

